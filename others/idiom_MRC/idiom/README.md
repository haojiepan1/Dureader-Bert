

## 成语阅读理解



#### 最终思路

* **当作完形填空(cloze style))的任务来做**

  

##### 数据处理：

* 每一条数据有多个有多个content，我们将它拆分----以每个content作为一条训练数据，每个content有多个  ‘*#idiom@@@@@@#*’ ，对于每一条训练数据，尽量保持文本信息（最大长度：512）。



##### 训练时：

* 将数据转化为id，经过模型后得到（batch_size, seq_len, 768）的矩阵，取每个 ‘*#idiom@@@@@@#*’  对应的向量 分别与3848个成语向量（随机初始化）做点积，取得分最高的作为该空的预测。

​     

##### 验证时：

* 对于每一条训练数据（多条content），通过模型，得到 每个 ‘*#idiom@@@@@@#*’  空对3848个成语的打分。之后，先通过10个候选成语排除其他成语的干扰，然后对每个空（ ‘*#idiom@@@@@@#*’ ）采取贪婪策略（**对于还没有被选中的空，每次选择得分最高的**）并保证不重复，来得到最后的预测结果。



#### 最终模型

* 经过许多实验，我们最终选择 xlnet + lstm(768, 3096) + LN + linear(3096, 3848) 作为最终模型 

  

#### 模型融合

1.  **投票：**7个模型进行投票，去票数最多的作为答案，若出现票数一样的情况，取最好的单模（7个模型中）的预测作为答案(7个模型为：bert、ernie、xlnet 以及以不同随机种子训练出来的模型)
2. **logits 融合：**将模型的输出按照比例相加后去预测答案，比例为每个模型的得分，归一化后的数值。



#### 决赛单模

* 我们采用知识蒸馏的方法，用4个模型（一个bert、一个ernie、两个不同随机种子训练的xlnet）蒸馏出一个单模，具体实现如下：
  1.  输入训练数据，将4个模型得到的logits，分别经过softmax，取平均后作为 label_1，而将训练数据的       label作为label_2；
  2.  计算loss：将训练数据经过我们的单模，得到logits，将经过softxmax后得到的logits与 label_1 计算交叉熵（要比均方差要好），得到 loss_1，然后将该logits经过分类层与 label_2计算交叉熵，得到loss_2；
  3. loss = 0.9 * loss_1 + 0.1 * loss_2
* 最终单模比蒸馏前提高了1个点
* 该想法全部参考论文：***Distilling the Knowledge in a Neural Network***



#### 实验 

- [ ] 在 xlnet 之后接什么层，我们做了一些尝试：

  * lstm > transformer > gru > linear > conv

- [ ] **蒸馏**

  * 有两种

  ​        1、直接对输出的logits蒸馏（上面提到的）

​               2、对这三个：embedding的输出、attentio矩阵中的得分矩阵、每层layer的输出，做均方差，对输出的                    logits做交叉熵。

​             最后选择第一种：训练相对快速、相对简单



#### 一些粗浅的理论

- [ ] 只拟合输出
  * 就拿输出的logits来说，teacher的logits包含了更多的信息，约束性也更强。有些人可能会有疑惑，觉得我的label要么是1要么是0，这样的约束才更强。其实不是的。
  * 对于三分类：土星、木星、桌子，假设我的ground truth是（0，1，0），这样就默认了这条训练数据是木星，土星和桌子是一样的。而如果teacher的预测是正确的话（0.3，0.6，0.1），这包含的信息就多了：我预测的答案是木星，但是比起桌子，它更可能是土星，这么一来约束性就起来了。但是如果teacher的预测试错的怎么办？它的预测也极大可能是（0.6，0.3，0.1），就算是错误的，这里面也包含了一些信息。但是如果你说teacher的预测是（0.1，0.3，0.6），那么很抱歉，模型无法判断这条数据，这可能是student超越teacher的一个难点。所以要适当加入ground truth作为一部分loss。



- [ ] 拟合embedding、transformer、layer、输出
  * 先说蒸馏出层数较少、隐藏节点少的：
    * 对这三个：embedding的输出、attention矩阵中的得分矩阵、每层layer的输出，做均方差，对输出的                    logits做交叉熵
    * 但是embedding的维度、隐藏节点个数不一样怎么计算均方差呢？
      * 对于小模型，embedding之后得到的矩阵再通过linear层映射到与teacher的embedding一样的维度，这样就可以计算均方差了；每层layer的输出同理。
        * 这样说的话，那么用来映射的linear层怎么办？是额外加一个用来将teacher维度映射到student维度的linear层映射回来，还是这个映射层只在训练时使用，推理时不用？CV是取后者，NLP不懂。
  * 如果说teacher的模型与student一样，那就不用映射了，但是这样的话无法使用多个结构不同的teacher来蒸馏了。